# ü¶ô Llama Integration - Quick Start Guide

## üöÄ Option 1: Automated Setup (Recommended)

Run the setup script to automatically configure everything:

```bash
python setup_llama.py
```

This will:
- Install required Python packages
- Guide you through Ollama installation
- Download recommended Llama models
- Test the integration

## üõ†Ô∏è Option 2: Manual Setup

### Step 1: Install Ollama

**Windows:**
1. Download from: https://ollama.ai/download
2. Run the installer
3. Restart your terminal

**Linux/Mac:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 2: Start Ollama Service

```bash
ollama serve
```
*Keep this running in a separate terminal*

### Step 3: Install Llama Models

Choose one or more models:

```bash
# Recommended for most users (4GB)
ollama pull llama2:7b

# Better quality, larger size (7GB)
ollama pull llama2:13b

# Latest version, excellent quality (4.7GB)
ollama pull llama3:8b

# Smaller, faster option (2GB)
ollama pull llama2:3b
```

### Step 4: Install Python Dependencies

```bash
pip install requests accelerate bitsandbytes
```

## üéØ Using Llama in Your PDF Summarizer

1. **Start Ollama service**: `ollama serve` (in separate terminal)
2. **Run your app**: `streamlit run pdf_summarizer.py`
3. **Select Llama model** from the dropdown in the sidebar
4. **Upload PDFs** and enjoy superior summarization!

## üìä Model Comparison

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama2:3b | 2GB | Fast | Good | Quick summaries |
| llama2:7b | 4GB | Medium | Excellent | Balanced performance |
| llama2:13b | 7GB | Slow | Superior | High-quality summaries |
| llama3:8b | 4.7GB | Medium | Excellent | Latest technology |

## üîß Troubleshooting

### "Ollama not found"
- Ensure Ollama is installed and in your PATH
- Restart your terminal after installation

### "Connection refused"
- Make sure `ollama serve` is running
- Check if port 11434 is available

### "Model not found"
- Verify model is installed: `ollama list`
- Pull the model: `ollama pull llama2:7b`

### "Out of memory"
- Use smaller model (llama2:3b)
- Close other applications
- Consider using CPU instead of GPU

## üéâ Benefits of Llama for Confidential Data

‚úÖ **Superior Context Understanding**: Better at maintaining relationships between concepts

‚úÖ **Comprehensive Summaries**: Preserves more important details

‚úÖ **Better Instruction Following**: Follows your specific requirements more accurately

‚úÖ **Domain Adaptability**: Can be fine-tuned for specific document types

‚úÖ **Complete Privacy**: Everything runs locally on your machine

## üöÄ Ready to Go!

Once setup is complete, your PDF summarizer will have:
- **Model Selection**: Choose between DistilBART and Llama models
- **Superior Quality**: Much better summaries for confidential data
- **Flexible Options**: Multiple Llama models to choose from
- **Fallback Support**: Automatic fallback to DistilBART if needed

**Your confidential data will get the best possible summarization!** üîí‚ú®